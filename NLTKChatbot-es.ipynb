{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construyendo un chatbot con NLTK desde cero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importamos librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_13235/3778381670.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpairwise\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcosine_similarity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import random\n",
    "import string\n",
    "import warnings\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importar NLTK "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/gustavo/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leemos el corpus con el texto para el chatbot\n",
    "\n",
    "Podemos utilizar una página de Wikipedia para crear el corpus. Podemos copiar el contenido en un fichero de texto.\n",
    "\n",
    "Debemos procesar el texto para para convertirlo en datos que el algoritmo pueda interpretar. Por ejemplo:\n",
    "\n",
    "* Convertir el texto a mayúsculas/minúsculas para que el algoritmo no diferencie los token.\n",
    "* Tokenización: NLTK permite tokenizar por frases (sentence tokenizer) y por palabras (word tokenizer)\n",
    "\n",
    "El paquete NLTK incluye:\n",
    "\n",
    "* Listas de stopwords\n",
    "* Stemming: obtener la raíz de una palabra. Por ejemplo, el algoritmo de Porter es un algoritmo de derivación de palabras y consiste en elimar las términaciones morfológicas comunes de las palabras en inglés, aunque es posible usarlo también en español. Es un proceso de normalización de términos muy usado en corpus basados en texto. Por ejemplo, texto lo transforma a text y canta a cant.\n",
    "* Lematización: relaciona una palabra flexionada o derivada con su forma canónica o lema. Por ejemplo, cantas, canto lo convierte a cantar. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open('chatbot-es.txt','r',errors = 'ignore')\n",
    "raw=f.read()\n",
    "raw = raw.lower()# converts to lowercase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_tokens = nltk.sent_tokenize(raw)# converts to list of sentences \n",
    "word_tokens = nltk.word_tokenize(raw)# converts to list of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesando\n",
    "\n",
    "Analizamos los tokens para normalizarlos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lemmer = nltk.stem.WordNetLemmatizer()\n",
    "stemmer = SnowballStemmer('spanish')\n",
    "\n",
    "def LemTokens(tokens):\n",
    "    #return [lemmer.lemmatize(token) for token in tokens]\n",
    "    return [stemmer.stem(token) for token in tokens]\n",
    "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
    "\n",
    "def LemNormalize(text):\n",
    "    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos una función para saludar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "GREETING_INPUTS = (\"hola\", \"que tal?\")\n",
    "GREETING_RESPONSES = [\"Buenos días\", \"hola\"]\n",
    "def greeting(sentence):\n",
    " \n",
    "    for word in sentence.split():\n",
    "        if word.lower() in GREETING_INPUTS:\n",
    "            return random.choice(GREETING_RESPONSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generamos la respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words\n",
    "\n",
    "Después del preproceso inicial, transformamos el texto en un vector (array) de números. El bag-of-words es la representación del texto que describe las ocurrencias de las palabras en un documento incluyendo el vocabulario de palabras y el número de ocurrencias. No se tiene en cuenta ni dónde ocurren ni el orden, solo tiene en cuenta si las palabras aparecen en el texto.\n",
    "\n",
    "En el caso de bag-of-words los documentos se consideran similares si contienen texto similar. Por ejemplo, {Gustavo, juega, al, fútbol} y vectorizamos el texto \"Gustavo juega\", tendríamos el siguiente vector: (1,1,0,0).\n",
    "\n",
    "\n",
    "**TF-IDF**\n",
    "\n",
    "Term Frequency: la frecuenca de las palabras en un documento.\n",
    "\n",
    "TF = (Número de veces que el término t aparece en un documento)/(Número de términos del documento)\n",
    "\n",
    "Inverse Document Frequency: mide la relevancia de una palabra para un documento en una colección.\n",
    "\n",
    "IDF = 1+log(N/n), donde, N el número de documentos y n es el número de documentos donde el térmimo t aparece.\n",
    "\n",
    "\n",
    "**Similitud del coseno**\n",
    "Es una medida de la similitud existente entre dos vectores. Mide el ángulo entre dos vectores.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para generar una respuesta los conceptos de similitud se tienen en cuenta. Definimos una función de respuesta que busca una o más palabras y devuelve una posible respuesta. Si no encuentra nada devuelve: ”Lo siento! No entiendo tu pregunta”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def response(user_response):\n",
    "    response=''\n",
    "    sent_tokens.append(user_response)\n",
    "    TfidfVec = TfidfVectorizer(tokenizer=LemNormalize, stop_words=stopwords.words(\"spanish\"))\n",
    "    tfidf = TfidfVec.fit_transform(sent_tokens)\n",
    "    vals = cosine_similarity(tfidf[-1], tfidf)\n",
    "    idx=vals.argsort()[0][-2]\n",
    "    flat = vals.flatten()\n",
    "    flat.sort()\n",
    "    req_tfidf = flat[-2]\n",
    "    if(req_tfidf==0):\n",
    "        response=robo_response+\"Lo siento! No entiendo tu pregunta\"\n",
    "        return response\n",
    "    else:\n",
    "        response = response+sent_tokens[idx]\n",
    "        return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, añadimos una función para solicitar el texto por pantalla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATI-UA: Mi nombre is ATI-UA y puedo contestar preguntas relacionadas con chatbots. Si deseas salir, inserta Adios!\n",
      "chatbot\n",
      "ATI-UA: estos servicios por chatbot ya existían por varios años en la app wechat.\n"
     ]
    }
   ],
   "source": [
    "flag=True\n",
    "print(\"ATI-UA: Mi nombre is ATI-UA y puedo contestar preguntas relacionadas con chatbots. Si deseas salir, inserta Adios!\")\n",
    "while(flag==True):\n",
    "    user_response = input()\n",
    "    user_response=user_response.lower()\n",
    "    if(user_response!='adios'):\n",
    "        if(user_response=='Gracias' or user_response=='Muchas gracias' ):\n",
    "            flag=False\n",
    "            print(\"ATI-UA: De nada.\")\n",
    "        else:\n",
    "            if(greeting(user_response)!=None):\n",
    "                print(\"ATI-UA: \"+greeting(user_response))\n",
    "            else:\n",
    "                print(\"ATI-UA: \",end=\"\")\n",
    "                print(response(user_response))\n",
    "                sent_tokens.remove(user_response)\n",
    "    else:\n",
    "        flag=False\n",
    "        print(\"ATI-UA: Hasta luego!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Referencias\n",
    "\n",
    "* https://github.com/parulnith/Building-a-Simple-Chatbot-in-Python-using-NLTK/blob/master/Chatbot.ipynb\n",
    "* https://medium.com/analytics-vidhya/building-a-simple-chatbot-in-python-using-nltk-7c8c8215ac6e\n",
    "* [Intro NLP](http://josearcosaneas.github.io/python/r/procesamiento/lenguaje/2017/01/02/procesamiento-lenguaje-natural-0.html#:~:text=El%20stemming%20consiste%20en%20extreaer,ayuda%20de%20la%20librer%C3%ADa%20NLTK.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
